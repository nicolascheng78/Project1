{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDX Neural Network Trading System\n",
    "## End-to-End Demonstration\n",
    "\n",
    "This notebook demonstrates the complete pipeline for building a research-grade neural network trading system for Indonesian equities (IDX) targeting >15% CAGR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import our modules\n",
    "sys.path.insert(0, '..')\n",
    "from idx_trading_system.data_loader import DataLoader\n",
    "from idx_trading_system.features import FeatureEngine, create_labels\n",
    "from idx_trading_system.models import create_model\n",
    "from idx_trading_system.portfolio import PortfolioConstructor\n",
    "from idx_trading_system.backtest import WalkForwardBacktest\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open('../idx_trading_system/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Universe: {len(config['universe']['tickers'])} tickers\")\n",
    "print(f\"Index: {config['universe']['index']}\")\n",
    "print(f\"Backtest splits: {len(config['backtest']['splits'])}\")\n",
    "print(f\"Target CAGR: {config['targets']['cagr']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Download and cache OHLCV data from Yahoo Finance for Indonesian equities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DataLoader(config['paths']['cache_dir'])\n",
    "\n",
    "# Load universe\n",
    "print(\"Loading data (this may take a few minutes on first run)...\")\n",
    "data_dict = data_loader.load_universe(config)\n",
    "\n",
    "equity_data = data_dict['equities']\n",
    "index_data = data_dict['index']\n",
    "\n",
    "print(f\"\\nEquity data shape: {equity_data.shape}\")\n",
    "print(f\"Tickers: {equity_data['ticker'].unique()}\")\n",
    "print(f\"Date range: {equity_data.index.min()} to {equity_data.index.max()}\")\n",
    "print(f\"\\nIndex data shape: {index_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize index\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(index_data.index, index_data['adj_close'])\n",
    "plt.title('Jakarta Composite Index (^JKSE)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Create technical indicators, cross-asset features, and regime features with no lookahead bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engine\n",
    "feature_engine = FeatureEngine(config)\n",
    "\n",
    "# Engineer features\n",
    "print(\"Engineering features...\")\n",
    "features = feature_engine.engineer_features(equity_data, index_data, shift_features=True)\n",
    "\n",
    "# Create labels\n",
    "features = create_labels(features, config['labels']['forward_periods'])\n",
    "\n",
    "print(f\"\\nFeatures shape: {features.shape}\")\n",
    "print(f\"\\nFeature columns ({len(features.columns)}):\")\n",
    "for col in features.columns[:20]:\n",
    "    print(f\"  - {col}\")\n",
    "print(\"  ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine features for one ticker\n",
    "ticker = 'BBCA.JK'\n",
    "ticker_features = features[features['ticker'] == ticker].copy()\n",
    "\n",
    "print(f\"Features for {ticker}:\")\n",
    "print(ticker_features[['returns', 'momentum_20d', 'volatility_20d', 'rsi', 'volume_zscore']].tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "Train baseline and neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train/val split for demonstration\n",
    "train_cutoff = pd.to_datetime('2014-12-31')\n",
    "val_cutoff = pd.to_datetime('2016-12-31')\n",
    "\n",
    "train_data = features[features.index <= train_cutoff]\n",
    "val_data = features[(features.index > train_cutoff) & (features.index <= val_cutoff)]\n",
    "\n",
    "# Select features\n",
    "exclude_cols = ['ticker', 'open', 'high', 'low', 'close', 'adj_close', 'volume', 'returns']\n",
    "exclude_cols += [col for col in features.columns if 'target' in col]\n",
    "feature_cols = [col for col in features.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Train period: {train_data.index.min()} to {train_data.index.max()}\")\n",
    "print(f\"Val period: {val_data.index.min()} to {val_data.index.max()}\")\n",
    "print(f\"Using {len(feature_cols)} features\")\n",
    "\n",
    "# Prepare data\n",
    "X_train = train_data[feature_cols].fillna(0).values\n",
    "y_train = train_data['target_return_20d'].fillna(0).values\n",
    "\n",
    "X_val = val_data[feature_cols].fillna(0).values\n",
    "y_val = val_data['target_return_20d'].fillna(0).values\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape}\")\n",
    "print(f\"Val set: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "print(\"Training XGBoost model...\")\n",
    "xgb_model = create_model('xgboost', model_type='regression', config=config)\n",
    "xgb_model.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"\\nModel training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "if hasattr(xgb_model, 'get_feature_importance'):\n",
    "    importance = xgb_model.get_feature_importance()\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 most important features:\")\n",
    "    print(feature_importance_df.head(10))\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(10), feature_importance_df['importance'].head(10))\n",
    "    plt.yticks(range(10), feature_importance_df['feature'].head(10))\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 10 Feature Importances')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Walk-Forward Backtesting\n",
    "\n",
    "Run strict time-series walk-forward backtest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "portfolio_constructor = PortfolioConstructor(config)\n",
    "backtester = WalkForwardBacktest(config)\n",
    "\n",
    "# Create a fresh model for backtesting\n",
    "model = create_model('xgboost', model_type='regression', config=config)\n",
    "\n",
    "# Run backtest\n",
    "print(\"Running walk-forward backtest...\\n\")\n",
    "results = backtester.run_backtest(\n",
    "    model,\n",
    "    feature_engine,\n",
    "    portfolio_constructor,\n",
    "    data_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"=\"*60)\n",
    "print(\"BACKTEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "avg_metrics = results['average_metrics_after_costs']\n",
    "\n",
    "print(f\"\\nAverage Performance Metrics (After Costs):\")\n",
    "print(f\"  CAGR: {avg_metrics['cagr']:.2%}\")\n",
    "print(f\"  Annual Volatility: {avg_metrics['annual_volatility']:.2%}\")\n",
    "print(f\"  Sharpe Ratio: {avg_metrics['sharpe_ratio']:.2f}\")\n",
    "print(f\"  Sortino Ratio: {avg_metrics['sortino_ratio']:.2f}\")\n",
    "print(f\"  Max Drawdown: {avg_metrics['max_drawdown']:.2%}\")\n",
    "print(f\"  Calmar Ratio: {avg_metrics['calmar_ratio']:.2f}\")\n",
    "print(f\"  Hit Rate: {avg_metrics['hit_rate']:.2%}\")\n",
    "\n",
    "target_cagr = config['targets']['cagr']\n",
    "print(f\"\\nTarget CAGR: {target_cagr:.2%}\")\n",
    "if avg_metrics['cagr'] >= target_cagr:\n",
    "    print(f\"✓ TARGET ACHIEVED!\")\n",
    "else:\n",
    "    print(f\"✗ Target not met. Consider hyperparameter tuning.\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot equity curves for each split\n",
    "fig, axes = plt.subplots(len(results['individual_results']), 1, figsize=(14, 4*len(results['individual_results'])))\n",
    "\n",
    "if len(results['individual_results']) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, result in enumerate(results['individual_results']):\n",
    "    returns = result['returns_after_costs']\n",
    "    equity_curve = (1 + returns).cumprod()\n",
    "    \n",
    "    ax = axes[i]\n",
    "    ax.plot(equity_curve.index, equity_curve.values)\n",
    "    ax.set_title(f\"Equity Curve - Split {i+1} ({result['split_period'][0].date()} to {result['split_period'][1].date()})\")\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Cumulative Return')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add metrics as text\n",
    "    metrics = result['metrics_after_costs']\n",
    "    text = f\"CAGR: {metrics['cagr']:.1%} | Sharpe: {metrics['sharpe_ratio']:.2f} | MaxDD: {metrics['max_drawdown']:.1%}\"\n",
    "    ax.text(0.02, 0.98, text, transform=ax.transAxes, \n",
    "           verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot drawdown for first split\n",
    "result = results['individual_results'][0]\n",
    "returns = result['returns_after_costs']\n",
    "cumulative = (1 + returns).cumprod()\n",
    "running_max = cumulative.expanding().max()\n",
    "drawdown = (cumulative - running_max) / running_max\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.fill_between(drawdown.index, drawdown.values, 0, alpha=0.3, color='red')\n",
    "plt.plot(drawdown.index, drawdown.values, color='red')\n",
    "plt.title('Drawdown - First Split')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Drawdown')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot turnover\n",
    "result = results['individual_results'][0]\n",
    "costs = result['costs']\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(costs.index, costs.values)\n",
    "plt.title('Transaction Costs Over Time - First Split')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cost')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average daily cost: {costs.mean():.4%}\")\n",
    "print(f\"Total cost impact: {costs.sum():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison\n",
    "\n",
    "Compare different models (optional - can be time-consuming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to run model comparison\n",
    "# models_to_test = ['linear', 'xgboost', 'lightgbm']\n",
    "# comparison_results = {}\n",
    "\n",
    "# for model_name in models_to_test:\n",
    "#     print(f\"\\nTesting {model_name}...\")\n",
    "#     model = create_model(model_name, model_type='regression', config=config)\n",
    "#     result = backtester.run_backtest(\n",
    "#         model,\n",
    "#         feature_engine,\n",
    "#         portfolio_constructor,\n",
    "#         data_dict\n",
    "#     )\n",
    "#     comparison_results[model_name] = result['average_metrics_after_costs']\n",
    "\n",
    "# # Display comparison\n",
    "# comparison_df = pd.DataFrame(comparison_results).T\n",
    "# print(\"\\nModel Comparison:\")\n",
    "# print(comparison_df[['cagr', 'sharpe_ratio', 'max_drawdown']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Data loading from Yahoo Finance with proper handling of adjusted prices\n",
    "2. Feature engineering with technical indicators and cross-asset features\n",
    "3. Model training with XGBoost (and other models available)\n",
    "4. Walk-forward backtesting with strict time-series splits\n",
    "5. Performance analysis with equity curves, drawdowns, and metrics\n",
    "\n",
    "**Next Steps:**\n",
    "- Run hyperparameter optimization if target CAGR not achieved\n",
    "- Test neural network models (LSTM, Transformer)\n",
    "- Perform feature ablation studies\n",
    "- Stress test across crisis periods (1997-98, 2008, 2020)\n",
    "- Implement ensemble methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
